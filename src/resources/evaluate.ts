// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

import { APIResource } from '../core/resource';
import { APIPromise } from '../core/api-promise';
import { RequestOptions } from '../internal/request-options';
import { path } from '../internal/utils/path';

export class Evaluate extends APIResource {
  /**
   * Use this endpoint to evaluate a model's input and output pair against selected
   * guardrail metrics
   */
  create(body: EvaluateCreateParams, options?: RequestOptions): APIPromise<Evaluation> {
    return this._client.post('/evaluate', { body, ...options });
  }

  /**
   * Use this endpoint to retrieve the evaluation record for a given evaluation ID
   */
  retrieve(evalID: string, options?: RequestOptions): APIPromise<Evaluation> {
    return this._client.get(path`/evaluate/${evalID}`, options);
  }
}

export interface Evaluation {
  /**
   * A unique evaluation ID.
   */
  eval_id: string;

  /**
   * Status of the evaluation.
   */
  evaluation_status: 'in_progress' | 'completed' | 'canceled' | 'queued' | 'failed';

  /**
   * A dictionary of inputs sent to the LLM to generate output. The dictionary must
   * contain at least `user_prompt` or `system_prompt` field. For
   * ground_truth_adherence guardrail metric, `ground_truth` should be provided.
   */
  model_input: Evaluation.ModelInput;

  /**
   * Output generated by the LLM to be evaluated.
   */
  model_output: string;

  /**
   * Run mode for the evaluation. The run mode allows the user to optimize for speed,
   * accuracy, and cost by determining which models are used to evaluate the event.
   */
  run_mode: 'precision_plus' | 'precision' | 'smart' | 'economy';

  /**
   * The time the evaluation was created in UTC.
   */
  created_at?: string;

  /**
   * The time the evaluation completed in UTC.
   */
  end_timestamp?: string;

  /**
   * Description of the error causing the evaluation to fail, if any.
   */
  error_message?: string;

  /**
   * The time the error causing the evaluation to fail was recorded.
   */
  error_timestamp?: string;

  /**
   * Evaluation result consisting of average scores and rationales for each of the
   * evaluated guardrail metrics.
   */
  evaluation_result?: { [key: string]: unknown };

  /**
   * Total cost of the evaluation.
   */
  evaluation_total_cost?: number;

  /**
   * An array of guardrail metrics that the model input and output pair will be
   * evaluated on.
   */
  guardrail_metrics?: Array<
    | 'correctness'
    | 'completeness'
    | 'instruction_adherence'
    | 'context_adherence'
    | 'ground_truth_adherence'
    | 'comprehensive_safety'
  >;

  /**
   * Model ID used to generate the output, like `gpt-4o` or `o3`.
   */
  model_used?: string;

  /**
   * The most recent time the evaluation was modified in UTC.
   */
  modified_at?: string;

  /**
   * An optional, user-defined tag for the evaluation.
   */
  nametag?: string;

  /**
   * Evaluation progress. Values range between 0 and 100; 100 corresponds to a
   * completed `evaluation_status`.
   */
  progress?: number;

  /**
   * The time the evaluation started in UTC.
   */
  start_timestamp?: string;
}

export namespace Evaluation {
  /**
   * A dictionary of inputs sent to the LLM to generate output. The dictionary must
   * contain at least `user_prompt` or `system_prompt` field. For
   * ground_truth_adherence guardrail metric, `ground_truth` should be provided.
   */
  export interface ModelInput {
    /**
     * The ground truth for evaluating Ground Truth Adherence guardrail.
     */
    ground_truth?: string;

    /**
     * The system prompt used to generate the output.
     */
    system_prompt?: string;

    /**
     * The user prompt used to generate the output.
     */
    user_prompt?: string;
  }
}

export interface EvaluateCreateParams {
  /**
   * A dictionary of inputs sent to the LLM to generate output. The dictionary must
   * contain at least `user_prompt` or `system_prompt` field. For
   * ground_truth_adherence guardrail metric, `ground_truth` should be provided.
   */
  model_input: EvaluateCreateParams.ModelInput;

  /**
   * Output generated by the LLM to be evaluated.
   */
  model_output: string;

  /**
   * Run mode for the evaluation. The run mode allows the user to optimize for speed,
   * accuracy, and cost by determining which models are used to evaluate the event.
   * Available run modes include `precision_plus`, `precision`, `smart`, and
   * `economy`. Defaults to `smart`.
   */
  run_mode: 'precision_plus' | 'precision' | 'smart' | 'economy';

  /**
   * An array of guardrail metrics that the model input and output pair will be
   * evaluated on. For non-enterprise users, these will be limited to the allowed
   * guardrail metrics.
   */
  guardrail_metrics?: Array<
    | 'correctness'
    | 'completeness'
    | 'instruction_adherence'
    | 'context_adherence'
    | 'ground_truth_adherence'
    | 'comprehensive_safety'
  >;

  /**
   * Model ID used to generate the output, like `gpt-4o` or `o3`.
   */
  model_used?: string;

  /**
   * An optional, user-defined tag for the evaluation.
   */
  nametag?: string;
}

export namespace EvaluateCreateParams {
  /**
   * A dictionary of inputs sent to the LLM to generate output. The dictionary must
   * contain at least `user_prompt` or `system_prompt` field. For
   * ground_truth_adherence guardrail metric, `ground_truth` should be provided.
   */
  export interface ModelInput {
    /**
     * The ground truth for evaluating Ground Truth Adherence guardrail.
     */
    ground_truth?: string;

    /**
     * The system prompt used to generate the output.
     */
    system_prompt?: string;

    /**
     * The user prompt used to generate the output.
     */
    user_prompt?: string;
  }
}

export declare namespace Evaluate {
  export { type Evaluation as Evaluation, type EvaluateCreateParams as EvaluateCreateParams };
}
