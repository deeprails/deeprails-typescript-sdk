// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

import { APIResource } from '../core/resource';

export class Evaluate extends APIResource {}

export interface Evaluation {
  /**
   * A unique evaluation ID.
   */
  eval_id: string;

  /**
   * Status of the evaluation.
   */
  evaluation_status: 'in_progress' | 'completed' | 'canceled' | 'queued' | 'failed';

  /**
   * A dictionary of inputs sent to the LLM to generate output. The dictionary must
   * contain at least a `user_prompt` field or a `system_prompt` field. For
   * ground_truth_adherence guardrail metric, `ground_truth` should be provided.
   */
  model_input: Evaluation.ModelInput;

  /**
   * Output generated by the LLM to be evaluated.
   */
  model_output: string;

  /**
   * Run mode for the evaluation. The run mode allows the user to optimize for speed,
   * accuracy, and cost by determining which models are used to evaluate the event.
   */
  run_mode: 'precision_plus' | 'precision' | 'smart' | 'economy';

  /**
   * The time the evaluation was created in UTC.
   */
  created_at?: string;

  /**
   * The time the evaluation completed in UTC.
   */
  end_timestamp?: string;

  /**
   * Description of the error causing the evaluation to fail, if any.
   */
  error_message?: string;

  /**
   * The time the error causing the evaluation to fail was recorded.
   */
  error_timestamp?: string;

  /**
   * Evaluation result consisting of average scores and rationales for each of the
   * evaluated guardrail metrics.
   */
  evaluation_result?: { [key: string]: unknown };

  /**
   * Total cost of the evaluation.
   */
  evaluation_total_cost?: number;

  /**
   * An array of guardrail metrics that the model input and output pair will be
   * evaluated on.
   */
  guardrail_metrics?: Array<
    | 'correctness'
    | 'completeness'
    | 'instruction_adherence'
    | 'context_adherence'
    | 'ground_truth_adherence'
    | 'comprehensive_safety'
  >;

  /**
   * Model ID used to generate the output, like `gpt-4o` or `o3`.
   */
  model_used?: string;

  /**
   * The most recent time the evaluation was modified in UTC.
   */
  modified_at?: string;

  /**
   * An optional, user-defined tag for the evaluation.
   */
  nametag?: string;

  /**
   * Evaluation progress. Values range between 0 and 100; 100 corresponds to a
   * completed `evaluation_status`.
   */
  progress?: number;

  /**
   * The time the evaluation started in UTC.
   */
  start_timestamp?: string;
}

export namespace Evaluation {
  /**
   * A dictionary of inputs sent to the LLM to generate output. The dictionary must
   * contain at least a `user_prompt` field or a `system_prompt` field. For
   * ground_truth_adherence guardrail metric, `ground_truth` should be provided.
   */
  export interface ModelInput {
    /**
     * The ground truth for evaluating Ground Truth Adherence guardrail.
     */
    ground_truth?: string;

    /**
     * The system prompt used to generate the output.
     */
    system_prompt?: string;

    /**
     * The user prompt used to generate the output.
     */
    user_prompt?: string;
  }
}

export declare namespace Evaluate {
  export { type Evaluation as Evaluation };
}
