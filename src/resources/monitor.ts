// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

import { APIResource } from '../core/resource';
import { APIPromise } from '../core/api-promise';
import { RequestOptions } from '../internal/request-options';
import { path } from '../internal/utils/path';

export class Monitor extends APIResource {
  /**
   * Use this endpoint to create a new monitor to evaluate model inputs and outputs
   * using guardrails
   */
  create(body: MonitorCreateParams, options?: RequestOptions): APIPromise<MonitorCreateResponse> {
    return this._client.post('/monitor', { body, ...options });
  }

  /**
   * Use this endpoint to retrieve the details and evaluations associated with a
   * specific monitor
   */
  retrieve(
    monitorID: string,
    query: MonitorRetrieveParams | null | undefined = {},
    options?: RequestOptions,
  ): APIPromise<MonitorDetailResponse> {
    return this._client.get(path`/monitor/${monitorID}`, { query, ...options });
  }

  /**
   * Use this endpoint to update the name, description, or status of an existing
   * monitor
   */
  update(
    monitorID: string,
    body: MonitorUpdateParams | null | undefined = {},
    options?: RequestOptions,
  ): APIPromise<MonitorUpdateResponse> {
    return this._client.put(path`/monitor/${monitorID}`, { body, ...options });
  }

  /**
   * Use this endpoint to submit a model input and output pair to a monitor for
   * evaluation
   */
  submitEvent(
    monitorID: string,
    body: MonitorSubmitEventParams,
    options?: RequestOptions,
  ): APIPromise<MonitorEventResponse> {
    return this._client.post(path`/monitor/${monitorID}/events`, { body, ...options });
  }
}

export interface MonitorCreateResponse {
  /**
   * The time the monitor was created in UTC.
   */
  created_at: string;

  /**
   * A unique monitor ID.
   */
  monitor_id: string;

  /**
   * Status of the monitor. Can be `active` or `inactive`. Inactive monitors no
   * longer record and evaluate events.
   */
  status: 'active' | 'inactive';
}

export interface MonitorDetailResponse {
  /**
   * A unique monitor ID.
   */
  monitor_id: string;

  /**
   * Name of this monitor.
   */
  name: string;

  /**
   * Status of the monitor. Can be `active` or `inactive`. Inactive monitors no
   * longer record and evaluate events.
   */
  status: 'active' | 'inactive';

  /**
   * An array of capabilities associated with this monitor.
   */
  capabilities?: Array<MonitorDetailResponse.Capability>;

  /**
   * The time the monitor was created in UTC.
   */
  created_at?: string;

  /**
   * Description of this monitor.
   */
  description?: string;

  /**
   * An array of all evaluations performed by this monitor. Each one corresponds to a
   * separate monitor event.
   */
  evaluations?: Array<MonitorDetailResponse.Evaluation>;

  /**
   * An array of files associated with this monitor.
   */
  files?: Array<MonitorDetailResponse.File>;

  /**
   * Contains five fields used for stats of this monitor: total evaluations,
   * completed evaluations, failed evaluations, queued evaluations, and in progress
   * evaluations.
   */
  stats?: MonitorDetailResponse.Stats;

  /**
   * The most recent time the monitor was modified in UTC.
   */
  updated_at?: string;
}

export namespace MonitorDetailResponse {
  export interface Capability {
    /**
     * The type of capability.
     */
    capability?: string;
  }

  export interface Evaluation {
    /**
     * Status of the evaluation.
     */
    evaluation_status: 'in_progress' | 'completed' | 'canceled' | 'queued' | 'failed';

    /**
     * A dictionary of inputs sent to the LLM to generate output. The dictionary must
     * contain at least a `user_prompt` field or a `system_prompt` field. For
     * ground_truth_adherence guardrail metric, `ground_truth` should be provided.
     */
    model_input: Evaluation.ModelInput;

    /**
     * Output generated by the LLM to be evaluated.
     */
    model_output: string;

    /**
     * Run mode for the evaluation. The run mode allows the user to optimize for speed,
     * accuracy, and cost by determining which models are used to evaluate the event.
     */
    run_mode: 'precision_plus' | 'precision' | 'smart' | 'economy';

    /**
     * The time the evaluation was created in UTC.
     */
    created_at?: string;

    /**
     * Error message if the evaluation failed.
     */
    error_message?: string;

    /**
     * Evaluation result consisting of average scores and rationales for each of the
     * evaluated guardrail metrics.
     */
    evaluation_result?: { [key: string]: unknown };

    /**
     * Total cost of the evaluation.
     */
    evaluation_total_cost?: number;

    /**
     * An array of guardrail metrics that the model input and output pair will be
     * evaluated on.
     */
    guardrail_metrics?: Array<
      | 'correctness'
      | 'completeness'
      | 'instruction_adherence'
      | 'context_adherence'
      | 'ground_truth_adherence'
      | 'comprehensive_safety'
    >;

    /**
     * An optional, user-defined tag for the evaluation.
     */
    nametag?: string;

    /**
     * Evaluation progress. Values range between 0 and 100; 100 corresponds to a
     * completed `evaluation_status`.
     */
    progress?: number;
  }

  export namespace Evaluation {
    /**
     * A dictionary of inputs sent to the LLM to generate output. The dictionary must
     * contain at least a `user_prompt` field or a `system_prompt` field. For
     * ground_truth_adherence guardrail metric, `ground_truth` should be provided.
     */
    export interface ModelInput {
      /**
       * The ground truth for evaluating Ground Truth Adherence guardrail.
       */
      ground_truth?: string;

      /**
       * The system prompt used to generate the output.
       */
      system_prompt?: string;

      /**
       * The user prompt used to generate the output.
       */
      user_prompt?: string;
    }
  }

  export interface File {
    /**
     * The ID of the file.
     */
    file_id?: string;

    /**
     * The name of the file.
     */
    file_name?: string;

    /**
     * The size of the file in bytes.
     */
    file_size?: number;
  }

  /**
   * Contains five fields used for stats of this monitor: total evaluations,
   * completed evaluations, failed evaluations, queued evaluations, and in progress
   * evaluations.
   */
  export interface Stats {
    /**
     * Number of evaluations that completed successfully.
     */
    completed_evaluations?: number;

    /**
     * Number of evaluations that failed.
     */
    failed_evaluations?: number;

    /**
     * Number of evaluations currently in progress.
     */
    in_progress_evaluations?: number;

    /**
     * Number of evaluations currently queued.
     */
    queued_evaluations?: number;

    /**
     * Total number of evaluations performed by this monitor.
     */
    total_evaluations?: number;
  }
}

export interface MonitorEventDetailResponse {
  /**
   * The capabilities associated with the monitor event.
   */
  capabilities?: Array<MonitorEventDetailResponse.Capability>;

  /**
   * The time spent on the evaluation in seconds.
   */
  eval_time?: string;

  /**
   * The result of the evaluation of the monitor event.
   */
  evaluation_result?: { [key: string]: unknown };

  /**
   * A unique monitor event ID.
   */
  event_id?: string;

  /**
   * The files associated with the monitor event.
   */
  files?: Array<MonitorEventDetailResponse.File>;

  /**
   * The guardrail metrics evaluated by the monitor event.
   */
  guardrail_metrics?: Array<string>;

  /**
   * The model input used to create the monitor event.
   */
  model_input?: { [key: string]: unknown };

  /**
   * The output evaluated by the monitor event.
   */
  model_output?: string;

  /**
   * Monitor ID associated with this event.
   */
  monitor_id?: string;

  /**
   * A human-readable tag for the monitor event.
   */
  nametag?: string;

  /**
   * The run mode used to evaluate the monitor event.
   */
  run_mode?: 'precision_plus' | 'precision' | 'smart' | 'economy';

  /**
   * Status of the monitor event's evaluation.
   */
  status?: 'in_progress' | 'completed' | 'canceled' | 'queued' | 'failed';

  /**
   * The time the monitor event was created in UTC.
   */
  timestamp?: string;
}

export namespace MonitorEventDetailResponse {
  export interface Capability {
    /**
     * The type of capability.
     */
    capability?: string;
  }

  export interface File {
    /**
     * The ID of the file.
     */
    file_id?: string;

    /**
     * The name of the file.
     */
    file_name?: string;

    /**
     * The size of the file in bytes.
     */
    file_size?: number;
  }
}

export interface MonitorEventResponse {
  /**
   * A unique monitor event ID.
   */
  event_id: string;

  /**
   * Monitor ID associated with this event.
   */
  monitor_id: string;

  /**
   * The time the monitor event was created in UTC.
   */
  created_at?: string;
}

export interface MonitorUpdateResponse {
  /**
   * The time the monitor was last modified in UTC.
   */
  modified_at: string;

  /**
   * A unique monitor ID.
   */
  monitor_id: string;

  /**
   * Status of the monitor. Can be `active` or `inactive`. Inactive monitors no
   * longer record and evaluate events.
   */
  status: 'active' | 'inactive';
}

export interface MonitorCreateParams {
  /**
   * An array of guardrail metrics that the model input and output pair will be
   * evaluated on. For non-enterprise users, these will be limited to `correctness`,
   * `completeness`, `instruction_adherence`, `context_adherence`,
   * `ground_truth_adherence`, and/or `comprehensive_safety`.
   */
  guardrail_metrics: Array<
    | 'correctness'
    | 'completeness'
    | 'instruction_adherence'
    | 'context_adherence'
    | 'ground_truth_adherence'
    | 'comprehensive_safety'
  >;

  /**
   * Name of the new monitor.
   */
  name: string;

  /**
   * Description of the new monitor.
   */
  description?: string;

  /**
   * An array of file IDs to search in the monitor's evaluations. Files must be
   * uploaded via the DeepRails API first.
   */
  file_search?: Array<string>;

  /**
   * Whether to enable web search for this monitor's evaluations. Defaults to false.
   */
  web_search?: boolean;
}

export interface MonitorRetrieveParams {
  /**
   * Limit the number of returned evaluations associated with this monitor. Defaults
   * to 10.
   */
  limit?: number;
}

export interface MonitorUpdateParams {
  /**
   * Description of the monitor.
   */
  description?: string;

  /**
   * Name of the monitor.
   */
  name?: string;

  /**
   * Status of the monitor. Can be `active` or `inactive`. Inactive monitors no
   * longer record and evaluate events.
   */
  status?: 'active' | 'inactive';
}

export interface MonitorSubmitEventParams {
  /**
   * A dictionary of inputs sent to the LLM to generate output. The dictionary must
   * contain at least a `user_prompt` field or a `system_prompt` field. For
   * ground_truth_adherence guardrail metric, `ground_truth` should be provided.
   */
  model_input: MonitorSubmitEventParams.ModelInput;

  /**
   * Output generated by the LLM to be evaluated.
   */
  model_output: string;

  /**
   * An optional, user-defined tag for the event.
   */
  nametag?: string;

  /**
   * Run mode for the monitor event. The run mode allows the user to optimize for
   * speed, accuracy, and cost by determining which models are used to evaluate the
   * event. Available run modes include `precision_plus`, `precision`, `smart`, and
   * `economy`. Defaults to `smart`.
   */
  run_mode?: 'precision_plus' | 'precision' | 'smart' | 'economy';
}

export namespace MonitorSubmitEventParams {
  /**
   * A dictionary of inputs sent to the LLM to generate output. The dictionary must
   * contain at least a `user_prompt` field or a `system_prompt` field. For
   * ground_truth_adherence guardrail metric, `ground_truth` should be provided.
   */
  export interface ModelInput {
    /**
     * The ground truth for evaluating Ground Truth Adherence guardrail.
     */
    ground_truth?: string;

    /**
     * The system prompt used to generate the output.
     */
    system_prompt?: string;

    /**
     * The user prompt used to generate the output.
     */
    user_prompt?: string;
  }
}

export declare namespace Monitor {
  export {
    type MonitorCreateResponse as MonitorCreateResponse,
    type MonitorDetailResponse as MonitorDetailResponse,
    type MonitorEventDetailResponse as MonitorEventDetailResponse,
    type MonitorEventResponse as MonitorEventResponse,
    type MonitorUpdateResponse as MonitorUpdateResponse,
    type MonitorCreateParams as MonitorCreateParams,
    type MonitorRetrieveParams as MonitorRetrieveParams,
    type MonitorUpdateParams as MonitorUpdateParams,
    type MonitorSubmitEventParams as MonitorSubmitEventParams,
  };
}
