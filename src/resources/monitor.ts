// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

import { APIResource } from '../core/resource';
import { APIPromise } from '../core/api-promise';
import { RequestOptions } from '../internal/request-options';
import { path } from '../internal/utils/path';

export class Monitor extends APIResource {
  /**
   * Use this endpoint to create a new monitor to evaluate model inputs and outputs
   * using guardrails
   */
  create(body: MonitorCreateParams, options?: RequestOptions): APIPromise<MonitorResponse> {
    return this._client.post('/monitor', { body, ...options });
  }

  /**
   * Use this endpoint to retrieve the details and evaluations associated with a
   * specific monitor
   */
  retrieve(
    monitorID: string,
    query: MonitorRetrieveParams | null | undefined = {},
    options?: RequestOptions,
  ): APIPromise<MonitorDetailResponse> {
    return this._client.get(path`/monitor/${monitorID}`, { query, ...options });
  }

  /**
   * Use this endpoint to update the name, description, or status of an existing
   * monitor
   */
  update(
    monitorID: string,
    body: MonitorUpdateParams | null | undefined = {},
    options?: RequestOptions,
  ): APIPromise<MonitorResponse> {
    return this._client.put(path`/monitor/${monitorID}`, { body, ...options });
  }

  /**
   * Use this endpoint to submit a model input and output pair to a monitor for
   * evaluation
   */
  submitEvent(
    monitorID: string,
    body: MonitorSubmitEventParams,
    options?: RequestOptions,
  ): APIPromise<MonitorEventResponse> {
    return this._client.post(path`/monitor/${monitorID}/events`, { body, ...options });
  }
}

export interface MonitorDetailResponse {
  /**
   * A unique monitor ID.
   */
  monitor_id: string;

  /**
   * Status of the monitor. Can be `active` or `inactive`. Inactive monitors no
   * longer record and evaluate events.
   */
  monitor_status: 'active' | 'inactive';

  /**
   * Name of this monitor.
   */
  name: string;

  /**
   * The time the monitor was created in UTC.
   */
  created_at?: string;

  /**
   * Description of this monitor.
   */
  description?: string;

  /**
   * An array of all evaluations performed by this monitor. Each one corresponds to a
   * separate monitor event.
   */
  evaluations?: Array<MonitorDetailResponse.Evaluation>;

  /**
   * Contains five fields used for stats of this monitor: total evaluations,
   * completed evaluations, failed evaluations, queued evaluations, and in progress
   * evaluations.
   */
  stats?: MonitorDetailResponse.Stats;

  /**
   * The most recent time the monitor was modified in UTC.
   */
  updated_at?: string;

  /**
   * User ID of the user who created the monitor.
   */
  user_id?: string;
}

export namespace MonitorDetailResponse {
  export interface Evaluation {
    /**
     * A unique evaluation ID.
     */
    eval_id: string;

    /**
     * Status of the evaluation.
     */
    evaluation_status: 'in_progress' | 'completed' | 'canceled' | 'queued' | 'failed';

    /**
     * A dictionary of inputs sent to the LLM to generate output. The dictionary must
     * contain at least a `user_prompt` field or a `system_prompt` field. For
     * ground_truth_adherence guardrail metric, `ground_truth` should be provided.
     */
    model_input: Evaluation.ModelInput;

    /**
     * Output generated by the LLM to be evaluated.
     */
    model_output: string;

    /**
     * Run mode for the evaluation. The run mode allows the user to optimize for speed,
     * accuracy, and cost by determining which models are used to evaluate the event.
     */
    run_mode: 'precision_plus' | 'precision' | 'smart' | 'economy';

    /**
     * The time the evaluation was created in UTC.
     */
    created_at?: string;

    /**
     * The time the evaluation completed in UTC.
     */
    end_timestamp?: string;

    /**
     * Description of the error causing the evaluation to fail, if any.
     */
    error_message?: string;

    /**
     * The time the error causing the evaluation to fail was recorded.
     */
    error_timestamp?: string;

    /**
     * Evaluation result consisting of average scores and rationales for each of the
     * evaluated guardrail metrics.
     */
    evaluation_result?: { [key: string]: unknown };

    /**
     * Total cost of the evaluation.
     */
    evaluation_total_cost?: number;

    /**
     * An array of guardrail metrics that the model input and output pair will be
     * evaluated on.
     */
    guardrail_metrics?: Array<
      | 'correctness'
      | 'completeness'
      | 'instruction_adherence'
      | 'context_adherence'
      | 'ground_truth_adherence'
      | 'comprehensive_safety'
    >;

    /**
     * Model ID used to generate the output, like `gpt-4o` or `o3`.
     */
    model_used?: string;

    /**
     * The most recent time the evaluation was modified in UTC.
     */
    modified_at?: string;

    /**
     * An optional, user-defined tag for the evaluation.
     */
    nametag?: string;

    /**
     * Evaluation progress. Values range between 0 and 100; 100 corresponds to a
     * completed `evaluation_status`.
     */
    progress?: number;

    /**
     * The time the evaluation started in UTC.
     */
    start_timestamp?: string;
  }

  export namespace Evaluation {
    /**
     * A dictionary of inputs sent to the LLM to generate output. The dictionary must
     * contain at least a `user_prompt` field or a `system_prompt` field. For
     * ground_truth_adherence guardrail metric, `ground_truth` should be provided.
     */
    export interface ModelInput {
      /**
       * The ground truth for evaluating Ground Truth Adherence guardrail.
       */
      ground_truth?: string;

      /**
       * The system prompt used to generate the output.
       */
      system_prompt?: string;

      /**
       * The user prompt used to generate the output.
       */
      user_prompt?: string;
    }
  }

  /**
   * Contains five fields used for stats of this monitor: total evaluations,
   * completed evaluations, failed evaluations, queued evaluations, and in progress
   * evaluations.
   */
  export interface Stats {
    /**
     * Number of evaluations that completed successfully.
     */
    completed_evaluations?: number;

    /**
     * Number of evaluations that failed.
     */
    failed_evaluations?: number;

    /**
     * Number of evaluations currently in progress.
     */
    in_progress_evaluations?: number;

    /**
     * Number of evaluations currently queued.
     */
    queued_evaluations?: number;

    /**
     * Total number of evaluations performed by this monitor.
     */
    total_evaluations?: number;
  }
}

export interface MonitorEventResponse {
  /**
   * A unique evaluation ID associated with this event.
   */
  evaluation_id: string;

  /**
   * A unique monitor event ID.
   */
  event_id: string;

  /**
   * Monitor ID associated with this event.
   */
  monitor_id: string;

  /**
   * The time the monitor event was created in UTC.
   */
  created_at?: string;
}

export interface MonitorResponse {
  /**
   * A unique monitor ID.
   */
  monitor_id: string;

  /**
   * Name of the monitor.
   */
  name: string;

  /**
   * The time the monitor was created in UTC.
   */
  created_at?: string;

  /**
   * Description of the monitor.
   */
  description?: string;

  /**
   * Status of the monitor. Can be `active` or `inactive`. Inactive monitors no
   * longer record and evaluate events.
   */
  monitor_status?: 'active' | 'inactive';

  /**
   * The most recent time the monitor was modified in UTC.
   */
  updated_at?: string;

  /**
   * User ID of the user who created the monitor.
   */
  user_id?: string;
}

export interface MonitorCreateParams {
  /**
   * Name of the new monitor.
   */
  name: string;

  /**
   * Description of the new monitor.
   */
  description?: string;
}

export interface MonitorRetrieveParams {
  /**
   * Limit the returned events associated with this monitor. Defaults to 10.
   */
  limit?: number;
}

export interface MonitorUpdateParams {
  /**
   * Description of the monitor.
   */
  description?: string;

  /**
   * Status of the monitor. Can be `active` or `inactive`. Inactive monitors no
   * longer record and evaluate events.
   */
  monitor_status?: 'active' | 'inactive';

  /**
   * Name of the monitor.
   */
  name?: string;
}

export interface MonitorSubmitEventParams {
  /**
   * An array of guardrail metrics that the model input and output pair will be
   * evaluated on. For non-enterprise users, these will be limited to `correctness`,
   * `completeness`, `instruction_adherence`, `context_adherence`,
   * `ground_truth_adherence`, and/or `comprehensive_safety`.
   */
  guardrail_metrics: Array<
    | 'correctness'
    | 'completeness'
    | 'instruction_adherence'
    | 'context_adherence'
    | 'ground_truth_adherence'
    | 'comprehensive_safety'
  >;

  /**
   * A dictionary of inputs sent to the LLM to generate output. The dictionary must
   * contain at least a `user_prompt` field or a `system_prompt` field. For
   * ground_truth_adherence guardrail metric, `ground_truth` should be provided.
   */
  model_input: MonitorSubmitEventParams.ModelInput;

  /**
   * Output generated by the LLM to be evaluated.
   */
  model_output: string;

  /**
   * Model ID used to generate the output, like `gpt-4o` or `o3`.
   */
  model_used?: string;

  /**
   * An optional, user-defined tag for the event.
   */
  nametag?: string;

  /**
   * Run mode for the monitor event. The run mode allows the user to optimize for
   * speed, accuracy, and cost by determining which models are used to evaluate the
   * event. Available run modes include `precision_plus`, `precision`, `smart`, and
   * `economy`. Defaults to `smart`.
   */
  run_mode?: 'precision_plus' | 'precision' | 'smart' | 'economy';
}

export namespace MonitorSubmitEventParams {
  /**
   * A dictionary of inputs sent to the LLM to generate output. The dictionary must
   * contain at least a `user_prompt` field or a `system_prompt` field. For
   * ground_truth_adherence guardrail metric, `ground_truth` should be provided.
   */
  export interface ModelInput {
    /**
     * The ground truth for evaluating Ground Truth Adherence guardrail.
     */
    ground_truth?: string;

    /**
     * The system prompt used to generate the output.
     */
    system_prompt?: string;

    /**
     * The user prompt used to generate the output.
     */
    user_prompt?: string;
  }
}

export declare namespace Monitor {
  export {
    type MonitorDetailResponse as MonitorDetailResponse,
    type MonitorEventResponse as MonitorEventResponse,
    type MonitorResponse as MonitorResponse,
    type MonitorCreateParams as MonitorCreateParams,
    type MonitorRetrieveParams as MonitorRetrieveParams,
    type MonitorUpdateParams as MonitorUpdateParams,
    type MonitorSubmitEventParams as MonitorSubmitEventParams,
  };
}
