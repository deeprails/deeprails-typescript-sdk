// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

import { maybeFilter } from 'deeprails-mcp/filtering';
import { Metadata, asTextContentResult } from 'deeprails-mcp/tools/types';

import { Tool } from '@modelcontextprotocol/sdk/types.js';
import Deeprails from 'deeprails';

export const metadata: Metadata = {
  resource: 'evaluate',
  operation: 'write',
  tags: [],
  httpMethod: 'post',
  httpPath: '/evaluate',
};

export const tool: Tool = {
  name: 'create_evaluate',
  description:
    "When using this tool, always use the `jq_filter` parameter to reduce the response size and improve performance.\n\nOnly omit if you're sure you don't need the data.\n\nUse this endpoint to evaluate a model's input and output pair against selected guardrail metrics\n\n\n# Response Schema\n```json\n{\n  $ref: '#/$defs/evaluation',\n  $defs: {\n    evaluation: {\n      type: 'object',\n      properties: {\n        eval_id: {\n          type: 'string',\n          description: 'A unique evaluation ID.'\n        },\n        evaluation_status: {\n          type: 'string',\n          description: 'Status of the evaluation.',\n          enum: [            'in_progress',\n            'completed',\n            'canceled',\n            'queued',\n            'failed'\n          ]\n        },\n        model_input: {\n          type: 'object',\n          description: 'A dictionary of inputs sent to the LLM to generate output. The dictionary must contain a `user_prompt` field and an optional `context` field.  Additional properties are allowed.\\n',\n          properties: {\n            user_prompt: {\n              type: 'string',\n              description: 'The user prompt used to generate the output.'\n            },\n            context: {\n              type: 'string',\n              description: 'Optional context supplied to the LLM when generating the output.'\n            }\n          },\n          required: [            'user_prompt'\n          ]\n        },\n        model_output: {\n          type: 'string',\n          description: 'Output generated by the LLM to be evaluated.'\n        },\n        run_mode: {\n          type: 'string',\n          description: 'Run mode for the evaluation.  The run mode allows the user to optimize for speed, accuracy, and cost by determining which models are used to evaluate the event.',\n          enum: [            'precision_plus',\n            'precision',\n            'smart',\n            'economy'\n          ]\n        },\n        created_at: {\n          type: 'string',\n          description: 'The time the evaluation was created in UTC.',\n          format: 'date-time'\n        },\n        end_timestamp: {\n          type: 'string',\n          description: 'The time the evaluation completed in UTC.',\n          format: 'date-time'\n        },\n        error_message: {\n          type: 'string',\n          description: 'Description of the error causing the evaluation to fail, if any.'\n        },\n        error_timestamp: {\n          type: 'string',\n          description: 'The time the error causing the evaluation to fail was recorded.',\n          format: 'date-time'\n        },\n        evaluation_result: {\n          type: 'object',\n          description: 'Evaluation result consisting of average scores and rationales for each of the evaluated guardrail metrics.',\n          additionalProperties: true\n        },\n        evaluation_total_cost: {\n          type: 'number',\n          description: 'Total cost of the evaluation.'\n        },\n        guardrail_metrics: {\n          type: 'array',\n          description: 'An array of guardrail metrics that the model input and output pair will be evaluated on.\\n',\n          items: {\n            type: 'string',\n            enum: [              'correctness',\n              'completeness',\n              'instruction_adherence',\n              'context_adherence',\n              'ground_truth_adherence',\n              'comprehensive_safety'\n            ]\n          }\n        },\n        model_used: {\n          type: 'string',\n          description: 'Model ID used to generate the output, like `gpt-4o` or `o3`.'\n        },\n        modified_at: {\n          type: 'string',\n          description: 'The most recent time the evaluation was modified in UTC.',\n          format: 'date-time'\n        },\n        nametag: {\n          type: 'string',\n          description: 'An optional, user-defined tag for the evaluation.'\n        },\n        progress: {\n          type: 'integer',\n          description: 'Evaluation progress.  Values range between 0 and 100; 100 corresponds to a completed `evaluation_status`.'\n        },\n        start_timestamp: {\n          type: 'string',\n          description: 'The time the evaluation started in UTC.',\n          format: 'date-time'\n        }\n      },\n      required: [        'eval_id',\n        'evaluation_status',\n        'model_input',\n        'model_output',\n        'run_mode'\n      ]\n    }\n  }\n}\n```",
  inputSchema: {
    type: 'object',
    properties: {
      model_input: {
        type: 'object',
        description:
          'A dictionary of inputs sent to the LLM to generate output.  This must contain a `user_prompt` field and an optional `context` field.  Additional properties are allowed.\n',
        properties: {
          user_prompt: {
            type: 'string',
          },
          context: {
            type: 'string',
          },
        },
        required: ['user_prompt'],
      },
      model_output: {
        type: 'string',
        description: 'Output generated by the LLM to be evaluated.',
      },
      run_mode: {
        type: 'string',
        description:
          'Run mode for the evaluation.  The run mode allows the user to optimize for speed, accuracy, and cost by determining which models are used to evaluate the event.  Available run modes include `precision_plus`, `precision`, `smart`, and `economy`.  Defaults to `smart`.',
        enum: ['precision_plus', 'precision', 'smart', 'economy'],
      },
      guardrail_metrics: {
        type: 'array',
        description:
          'An array of guardrail metrics that the model input and output pair will be evaluated on.  For non-enterprise users, these will be limited to the allowed guardrail metrics.\n',
        items: {
          type: 'string',
          enum: [
            'correctness',
            'completeness',
            'instruction_adherence',
            'context_adherence',
            'ground_truth_adherence',
            'comprehensive_safety',
          ],
        },
      },
      model_used: {
        type: 'string',
        description: 'Model ID used to generate the output, like `gpt-4o` or `o3`.',
      },
      nametag: {
        type: 'string',
        description: 'An optional, user-defined tag for the evaluation.',
      },
      jq_filter: {
        type: 'string',
        title: 'jq Filter',
        description:
          'A jq filter to apply to the response to include certain fields. Consult the output schema in the tool description to see the fields that are available.\n\nFor example: to include only the `name` field in every object of a results array, you can provide ".results[].name".\n\nFor more information, see the [jq documentation](https://jqlang.org/manual/).',
      },
    },
    required: ['model_input', 'model_output', 'run_mode'],
  },
  annotations: {},
};

export const handler = async (client: Deeprails, args: Record<string, unknown> | undefined) => {
  const { jq_filter, ...body } = args as any;
  return asTextContentResult(await maybeFilter(jq_filter, await client.evaluate.create(body)));
};

export default { metadata, tool, handler };
